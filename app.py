# from elasticsearch import Elasticsearch
from langchain_core.tools import tool
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.models import VectorizableTextQuery
from langchain_openai import AzureChatOpenAI
from langgraph.graph import StateGraph, MessagesState, END, START
from langgraph.prebuilt import ToolNode
from typing import Dict, List
import os
from dotenv import load_dotenv
from openai import AzureOpenAI
import streamlit as st
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from sample_data import NODE_METRIC, POD_METRIC, APM_SAMPLE, EVENT_LOG, NODE_POD_MAP


load_dotenv()
os.environ["LANGCHAIN_TRACING_V2"] = "false"

# AI Search service
search_api_key = os.getenv("AZURE_SEARCH_API_KEY")
search_endpoint = "https://kkoldduck-search-000001.search.windows.net"
index_name = "rag-1750308489612"

search_client = SearchClient(
    endpoint=search_endpoint,
    index_name=index_name,
    credential=AzureKeyCredential(search_api_key)
)

deployment = "dev-gpt-4o-mini"
openai_api_key = os.getenv("AZURE_OPENAI_API_KEY")
llm = AzureChatOpenAI(
    azure_deployment=deployment,
    api_version="2024-12-01-preview",
    azure_endpoint="https://kkoldduck-openai-002.openai.azure.com",
    temperature=0.,
    api_key=openai_api_key
)

# ====================================
# íŒŒë¼ë¯¸í„° ì¶”ì¶œ ì²´ì¸ (LLM ê¸°ë°˜)
# ====================================
extract_prompt = ChatPromptTemplate.from_messages([
    ("system", "ë„ˆëŠ” ì¿ ë²„ë„¤í‹°ìŠ¤ ì¥ì•  ë¶„ì„ê¸°ì•¼. ì§ˆë¬¸ì—ì„œ ë‹¤ìŒì„ JSONìœ¼ë¡œ ì¶”ì¶œí•´ì¤˜:\n"
               "- object_type: node | pod | service\n"
               "- object_name: ì´ë¦„\n"
               "- metric: cpu | memory | disk | latency ë“±\n"
               "- timerange: 30m, 1h, 24h ê°™ì€ ìƒëŒ€ ì‹œê°„"),
    ("human", "{question}")
])
extract_chain = extract_prompt | llm | JsonOutputParser()

# ====================================
# ì¿¼ë¦¬ ìƒì„± í•¨ìˆ˜
# ====================================
def build_metric_query(params: dict) -> dict:
    timerange = params.get("timerange", "1h")
    return {
        "index": "k8s-metric-*",
        "size": 100,
        "query": {
            "bool": {
                "must": [
                    {"term": {"node.name.keyword": params["object_name"]}},
                    {"range": {"@timestamp": {"gte": f"now-{timerange}"}}}
                ]
            }
        },
        "sort": [{"@timestamp": {"order": "desc"}}]
    }

def build_apm_query(params: dict) -> dict:
    timerange = params.get("timerange", "1h")
    return {
        "index": "apm-*",
        "size": 0,
        "query": {
            "bool": {
                "must": [
                    {"term": {"service.name.keyword": params["object_name"]}},
                    {"range": {"@timestamp": {"gte": f"now-{timerange}"}}}
                ]
            }
        },
        "aggs": {
            "avg_latency": {"avg": {"field": "transaction.duration.us"}},
            "error_rate": {"filter": {"term": {"event.outcome": "failure"}}}
        }
    }

summarize_prompt = ChatPromptTemplate.from_template(
    "ë‹¤ìŒ ì§ˆë¬¸ì„ RAG ê²€ìƒ‰ì— ì í•©í•˜ë„ë¡ í•œ ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ìš”ì•½í•´ì¤˜. ë„ˆë¬´ êµ¬ì²´ì ì¸ ìˆ«ìëŠ” ìƒëµí•˜ê³  'ê³¼ë¶€í•˜', 'ì§€ì—°', 'ì‹¤íŒ¨' ê°™ì€ ì¥ì•  í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±í•´ì¤˜:\n\nì§ˆë¬¸: {query}\n\nìš”ì•½:"
)
summarize_chain = summarize_prompt | llm | StrOutputParser()

# Tool ì •ì˜ : rag ê²€ìƒ‰ / ë©”íŠ¸ë¦­ ì¡°íšŒ
@tool
def retrieve_rag(query: str, top_k: int = 3) -> list[str]:
    """
    ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ Azure Cognitive Searchì˜ ë²¡í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì—¬
    ìœ ì‚¬í•œ Kubernetes ì¥ì•  ì‚¬ë¡€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ê²½ìš° ì¼ë°˜ì ì¸ ì‚¬ë¡€ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤

    Args:
        query (str): ê²€ìƒ‰í•  ìš”ì•½ ë° íˆìŠ¤í† ë¦¬ ë¬¸ìì—´
        k (int): ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜ (default=3)
    Returns:
        list[str]: ìœ ì‚¬ ì‚¬ë¡€ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    print(f"[DEBUG] retrieve_rag called with query = {query}, top_k = {top_k}")
    vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=top_k,
                                     fields="text_vector")
    results = search_client.search(search_text=query, vector_queries=[vector_query],
                                filter=None, top=top_k)
    return [
        (doc["@search.score"], doc["chunk"])
        for doc in results
    ]


@tool
def list_services_on_node(question: str) -> dict:
    """
    íŠ¹ì • ë…¸ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì¸ ì„œë¹„ìŠ¤ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    params = extract_chain.invoke({"question": question})
    print(f"params: {params}")

    node_name = params.get("object_name", "").lower()  # ì†Œë¬¸ìë¡œ í‘œì¤€í™”
    print(f"[DEBUG] list_services_on_node tool called with node_name = {node_name}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ë…¸ë“œë³„ ì„œë¹„ìŠ¤ ìƒ˜í”Œ ë°ì´í„°
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _FAKE_NODE_SERVICE_MAP = {
        "node-a": ["checkout-service", "payment-api"],
        "node-b": ["order-service", "auth-api", "inventory-service"],
        "node-c": ["analytics-batch", "report-generator"],
    }

    services = _FAKE_NODE_SERVICE_MAP.get(node_name)
    if services is None:
        return {
            "data": [],
            "status_code": 1,
            "message": f"ë…¸ë“œ '{node_name}'ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        }

    return {
        "data": services,
        "status_code": 0
    }

@tool
def metric_search(question: str) -> Dict:
    """íŠ¹ì • ë…¸ë“œ í˜¹ì€ íŒŒë“œì— ëŒ€í•œ ë©”íŠ¸ë¦­ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    params = extract_chain.invoke({"question": question})
    print(f"[DEBUG] metric_search params: {params}")

    obj_type = (params.get("object_type") or "").strip().lower()
    name = (params.get("object_name") or "").strip().lower()
    metrics = params.get("metric")

    # metricì€ list or str or None ì²˜ë¦¬
    if isinstance(metrics, str):
        metrics = [metrics]
    elif metrics is None:
        metrics = []

    source = POD_METRIC if obj_type == "pod" else NODE_METRIC
    available = source.get(name)

    if not available:
        return {"data": {}, "status_code": 1, "message": f"{name}ì— ëŒ€í•œ ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

    result = {}
    if metrics:
        for m in metrics:
            val = available.get(f"{m}_usage") or available.get(m)
            if val: result[m] = val
    else:
        result = available  # metric ë¯¸ì§€ì • ì‹œ ì „ì²´ ë°˜í™˜

    return {"data": result, "status_code": 0} if result else {
        "data": {}, "status_code": 1, "message": f"{name}ì˜ ìš”ì²­í•œ ë©”íŠ¸ë¦­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    }

@tool
def apm_search(question: str) -> dict:
    """íŠ¹ì • ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì„±ëŠ¥ ì§€í‘œë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    params = extract_chain.invoke({"question": question})
    print(f"[DEBUG] apm_search called with params: {params}")
    service = params.get("object_name", "").lower()
    return {"data": APM_SAMPLE.get(service, {}), "status_code": 0}

@tool
def event_search(question: str) -> dict:
    """íŠ¹ì • íŒŒë“œë‚˜ ë…¸ë“œ, PVCì— ëŒ€í•œ ì´ë²¤íŠ¸ ë¡œê·¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    params = extract_chain.invoke({"question": question})
    print(f"[DEBUG] event_search called with params: {params}")
    target = params.get("object_name")          # pod ì´ë¦„ ë˜ëŠ” pvc ì´ë¦„
    events = EVENT_LOG.get(target, [])
    return {"data": events, "status_code": 0}

@tool
def list_pods_on_node(question: str) -> list:
    """
    íŠ¹ì • ë…¸ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì¸ íŒŒë“œ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    params = extract_chain.invoke({"question": question})
    print(f"[DEBUG] list_pods_on_node called with params: {params}")
    node_name = params.get("object_name")    
    return NODE_POD_MAP.get(node_name, [])

class AgentState(MessagesState):
    pass

tools = [retrieve_rag, apm_search, metric_search, list_services_on_node, event_search, list_pods_on_node]
agent_engine = llm.bind_tools(tools=tools)
tool_node = ToolNode(tools)


# ëª¨ë¸ í˜¸ì¶œ ë…¸ë“œ
def call_model(state: AgentState):
    sys_prompt = SystemMessage(
        "ë‹¹ì‹ ì€ ì¿ ë²„ë„¤í‹°ìŠ¤ SRE ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ë„êµ¬ë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n"
        "- metric_search: ë…¸ë“œë‚˜ íŒŒë“œì˜ ìƒíƒœë¥¼ ë¶„ì„í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"
        "- apm_search: ì„œë¹„ìŠ¤ì˜ ì„±ëŠ¥ ì§€í‘œ(ì§€ì—° ì‹œê°„, ì—ëŸ¬ìœ¨ ë“±)ë¥¼ ë¶„ì„í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"
        "- list_services_on_node: íŠ¹ì • ë…¸ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì¸ ì„œë¹„ìŠ¤ ëª©ë¡ì´ ê¶ê¸ˆí•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"
        "- retrieve_rag: ê³¼ê±°ì˜ ìœ ì‚¬ ì‚¬ë¡€ê°€ ê¶ê¸ˆí•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\n"
        "ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸í•´ì„œ ë¨¼ì € í™•ë³´í•˜ì„¸ìš”.\n"
        "ì§ˆë¬¸ì— ë”°ë¼ í•„ìš”í•œ ë„êµ¬ë¥¼ í•˜ë‚˜ ì´ìƒ ì ì ˆí•˜ê²Œ í˜¸ì¶œí•˜ê³ , ë„êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤."
        "ì´ë¯¸ í˜¸ì¶œëœ ë„êµ¬ë¥¼ ë°˜ë³µ í˜¸ì¶œí•˜ê¸°ë³´ë‹¤ëŠ” ë‹¤ìŒ ë¶„ì„ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ì„¸ìš”."
        "âš ï¸ ê°™ì€ ë„êµ¬ë¥¼ ë°˜ë³µì ìœ¼ë¡œ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”. ì´ë¯¸ ì‚¬ìš©í•œ ë„êµ¬ëŠ” ì¬ì‚¬ìš©í•˜ì§€ ë§ê³ , ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”."
    )
    response = agent_engine.invoke([sys_prompt] + state["messages"])
    return {"messages": [response]}

# íˆ´ í˜¸ì¶œ í›„ ë‹¤ì‹œ ëª¨ë¸ë¡œ ëŒì•„ì˜¬ì§€ ê²°ì •
def should_continue(state: AgentState):
    last_msg = state["messages"][-1]

    # ëª¨ë¸ì´ íˆ´ í˜¸ì¶œì„ ëª…í™•íˆ ìš”ì²­í–ˆì„ ë•Œë§Œ ê³„ì†
    if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
        return "tools"

    # ë„êµ¬ í˜¸ì¶œ ëë‚¬ê³ , ì‘ë‹µ ë©”ì‹œì§€ê°€ assistantë¼ë©´ ì¢…ë£Œ
    return END

# --------- Define the graph
workflow = StateGraph(AgentState)

workflow.add_node("agent", call_model)
workflow.add_node("tools", tool_node)

workflow.set_entry_point("agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    ["tools", END]
)
workflow.add_edge("tools", "agent")
graph = workflow.compile()

# ====================================
# Streamlit UI (ChatGPT ìŠ¤íƒ€ì¼)
# ====================================
content = "You are a kubernetes SRE assistant that helps users analyze and resolve issues related to Kubernetes clusters."
if "ui_messages" not in st.session_state:
    st.session_state.ui_messages = [
        {"role": "system",
         "content": content
        },
    ]
if "graph_messages" not in st.session_state:
    st.session_state.graph_messages = []     # LangChain Message ì „ìš©

# -----------------------------------------------
# UI: íƒ€ì´í‹€ - íˆìŠ¤í† ë¦¬ ì¶œë ¥
st.title("ğŸ›  ì¥ì•  ì›ì¸ ë¶„ì„ ì‹œìŠ¤í…œ")
st.write("ì¥ì•  ìƒí™©ì„ ì…ë ¥í•˜ë©´ ë©”íŠ¸ë¦­ ì¡°íšŒ â†’ ìš”ì•½ â†’ ìœ ì‚¬ì‚¬ë¡€ ê²€ìƒ‰ â†’ ì¡°ì¹˜ë°©ì•ˆì„ ì¶”ì²œí•©ë‹ˆë‹¤.")

for m in st.session_state.ui_messages:
    st.chat_message(m["role"]).write(m["content"])

# -----------------------------------------------
# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_input := st.chat_input("ì¥ì•  ìƒí™©ì„ ì…ë ¥í•˜ì„¸ìš”:"):
    # 1) UI íˆìŠ¤í† ë¦¬ì— ë°˜ì˜
    st.session_state.ui_messages.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)

    # 2) ê·¸ë˜í”„ìš© íˆìŠ¤í† ë¦¬ì— ë°˜ì˜
    st.session_state.graph_messages.append(HumanMessage(content=user_input))

    # 3) LangGraph ì‹¤í–‰
    with st.spinner("ë¶„ì„ ì¤‘..."):
        state = graph.invoke({"messages": st.session_state.graph_messages})
        ai_msg = state["messages"][-1]          # LangChain AIMessage
        answer = ai_msg.content

    # 4) ë‘ íˆìŠ¤í† ë¦¬ì— AI ì‘ë‹µ push
    st.session_state.graph_messages.append(ai_msg)
    st.session_state.ui_messages.append({"role": "assistant", "content": answer})

    # 5) UI ì¶œë ¥
    st.chat_message("assistant").write(answer)

